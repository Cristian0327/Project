{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# 1Ô∏è‚É£ Configuraci√≥n para Google Colab\n",
        "# ========================\n",
        "\n",
        "# Instalar dependencias si es necesario\n",
        "!pip install keras-tuner\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras_tuner as kt\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import random\n",
        "from google.colab import drive\n",
        "\n",
        "# ========================\n",
        "# 2Ô∏è‚É£ Montar Google Drive y cargar archivos\n",
        "# ========================\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "dataset_path = \"/content/drive/My Drive/Parcial 1\"\n",
        "\n",
        "train_images = np.load(f\"{dataset_path}/kmnist-train-imgs.npz\")[\"arr_0\"]\n",
        "train_labels = np.load(f\"{dataset_path}/kmnist-train-labels.npz\")[\"arr_0\"]\n",
        "test_images = np.load(f\"{dataset_path}/kmnist-test-imgs.npz\")[\"arr_0\"]\n",
        "test_labels = np.load(f\"{dataset_path}/kmnist-test-labels.npz\")[\"arr_0\"]\n",
        "\n",
        "# Normalizaci√≥n\n",
        "train_images = train_images.astype(\"float32\") / 255.0\n",
        "test_images = test_images.astype(\"float32\") / 255.0\n",
        "\n",
        "# Convertir etiquetas a one-hot encoding\n",
        "train_labels = to_categorical(train_labels, 10)\n",
        "test_labels = to_categorical(test_labels, 10)\n",
        "\n",
        "# Divisi√≥n en Entrenamiento (85%) y Validaci√≥n (15%)\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(\n",
        "    train_images, train_labels, test_size=0.15, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "# ========================\n",
        "# 3Ô∏è‚É£ Data Augmentation\n",
        "# ========================\n",
        "\n",
        "train_images_reshaped = train_images.reshape(-1, 28, 28, 1)\n",
        "val_images_reshaped   = val_images.reshape(-1, 28, 28, 1)\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    zoom_range=0.10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1\n",
        ")\n",
        "\n",
        "# ========================\n",
        "# 4Ô∏è‚É£ Modelo MLP con Hiperpar√°metros\n",
        "# ========================\n",
        "\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(28, 28, 1)))\n",
        "    n_neuronas_1 = hp.Choice('n_neuronas_1', [256, 384, 512])\n",
        "    model.add(Dense(n_neuronas_1, activation='relu', kernel_regularizer=l2(1e-5)))\n",
        "    model.add(BatchNormalization())\n",
        "    dropout_1 = hp.Choice('dropout_1', [0.3, 0.4, 0.5])\n",
        "    model.add(Dropout(dropout_1))\n",
        "    n_neuronas_2 = hp.Choice('n_neuronas_2', [128, 256, 384])\n",
        "    model.add(Dense(n_neuronas_2, activation='relu', kernel_regularizer=l2(1e-5)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    lr = 5e-4\n",
        "    model.compile(optimizer=Adam(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# ========================\n",
        "# 5Ô∏è‚É£ B√∫squeda de hiperpar√°metros con Keras Tuner\n",
        "# ========================\n",
        "\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=9,\n",
        "    executions_per_trial=1,\n",
        "    project_name='kmnist_hyperparam_tuning'\n",
        ")\n",
        "\n",
        "# ========================\n",
        "# 6Ô∏è‚É£ Entrenamiento del modelo con los mejores hiperpar√°metros\n",
        "# ========================\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)\n",
        "\n",
        "tuner.search(\n",
        "    datagen.flow(train_images_reshaped, train_labels, batch_size=64),\n",
        "    validation_data=(val_images_reshaped, val_labels),\n",
        "    epochs=10,\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")\n",
        "\n",
        "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "final_model = tuner.hypermodel.build(best_hp)\n",
        "\n",
        "history = final_model.fit(\n",
        "    datagen.flow(train_images_reshaped, train_labels, batch_size=64),\n",
        "    validation_data=(val_images_reshaped, val_labels),\n",
        "    epochs=20,\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")\n",
        "\n",
        "# ========================\n",
        "# 7Ô∏è‚É£ Evaluaci√≥n y visualizaci√≥n de resultados\n",
        "# ========================\n",
        "\n",
        "test_images_reshaped = test_images.reshape(-1, 28, 28, 1)\n",
        "test_loss, test_acc = final_model.evaluate(test_images_reshaped, test_labels)\n",
        "print(f\"\\n‚úÖ Accuracy Final en Test: {test_acc:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Entrenamiento')\n",
        "plt.plot(history.history['val_accuracy'], label='Validaci√≥n')\n",
        "plt.xlabel('√âpocas')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Precisi√≥n del Modelo')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Entrenamiento')\n",
        "plt.plot(history.history['val_loss'], label='Validaci√≥n')\n",
        "plt.xlabel('√âpocas')\n",
        "plt.ylabel('P√©rdida')\n",
        "plt.title('P√©rdida del Modelo')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# ========================\n",
        "# 8Ô∏è‚É£ Matriz de Confusi√≥n y Reporte de Clasificaci√≥n\n",
        "# ========================\n",
        "\n",
        "y_pred = final_model.predict(test_images_reshaped)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(test_labels, axis=1)\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=\"Blues\")\n",
        "plt.title(\"Matriz de Confusi√≥n\")\n",
        "plt.xlabel(\"Predicho\")\n",
        "plt.ylabel(\"Verdadero\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìå Reporte de Clasificaci√≥n:\\n\")\n",
        "print(classification_report(y_true, y_pred_classes))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jcZv0RZH7rN",
        "outputId": "0daea705-8746-403a-b481-a7b81c4a1c33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 2 Complete [00h 02m 48s]\n",
            "val_accuracy: 0.9617778062820435\n",
            "\n",
            "Best val_accuracy So Far: 0.9617778062820435\n",
            "Total elapsed time: 00h 05m 34s\n",
            "\n",
            "Search: Running Trial #3\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "384               |512               |n_neuronas_1\n",
            "0.4               |0.3               |dropout_1\n",
            "128               |384               |n_neuronas_2\n",
            "\n",
            "Epoch 1/10\n",
            "\u001b[1m797/797\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.5566 - loss: 1.3927 - val_accuracy: 0.8904 - val_loss: 0.3864 - learning_rate: 5.0000e-04\n",
            "Epoch 2/10\n",
            "\u001b[1m797/797\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - accuracy: 0.7557 - loss: 0.7608 - val_accuracy: 0.9138 - val_loss: 0.3081 - learning_rate: 5.0000e-04\n",
            "Epoch 3/10\n",
            "\u001b[1m427/797\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.7920 - loss: 0.6617"
          ]
        }
      ]
    }
  ]
}